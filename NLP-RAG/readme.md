# ğŸš€ Retrieval-Augmented Generation (RAG) ğŸ“š

Le Retrieval-Augmented Generation (RAG) est une approche avancÃ©e de l'intelligence artificielle qui combine la **gÃ©nÃ©ration de texte**  avec la **rÃ©cupÃ©ration d'informations pertinentes** ğŸ” Ã  partir d'une base de connaissances. Cela permet d'amÃ©liorer la **prÃ©cision**  et la **fiabilitÃ©**  des rÃ©ponses gÃ©nÃ©rÃ©es par les modÃ¨les de langage (**LLM**).

## âš™ï¸ Fonctionnement de RAG
1. **RequÃªte utilisateur** ğŸ—£ï¸ : Une question ou un texte est soumis au systÃ¨me.
2. **RÃ©cupÃ©ration d'informations** ğŸ“‚ : Le systÃ¨me recherche les documents les plus pertinents dans une base vectorielle.
3. **GÃ©nÃ©ration de rÃ©ponse** ğŸ¤– : Un modÃ¨le de langage (**LLM**) utilise ces documents comme contexte pour gÃ©nÃ©rer une rÃ©ponse plus pertinente et prÃ©cise.

## Types de fichiers pris en charge
Un systÃ¨me RAG peut traiter plusieurs types de fichiers, notamment :
- **Texte brut** (`.txt`)
- **Documents Word** (`.docx`)
- **PDF** (`.pdf`)
- **Pages HTML** (`.html`)
- **Formats JSON et CSV** pour des bases de donnÃ©es structurÃ©es.

## Segmentation du texte (Text Splitter)
Un **text splitter** permet de diviser un document en sections exploitables par le modÃ¨le. DiffÃ©rentes stratÃ©gies existent :
- **BasÃ© sur le nombre de tokens** : DÃ©coupe le texte en fonction dâ€™un nombre fixe de tokens.
- **BasÃ© sur les sauts de ligne** : Segmente un document selon ses paragraphes.
- **BasÃ© sur la structure** : Divise un document selon des titres et sous-titres.
- 
## ğŸ›ï¸ Choix des Bases Vectorielles
Les bases vectorielles sont utilisÃ©es pour stocker et rechercher des reprÃ©sentations numÃ©riques des documents.

### 1ï¸âƒ£ **ChromaDB** ğŸ—ï¸
ğŸ”¹ **Description** : Chroma est une base de donnÃ©es vectorielle moderne, optimisÃ©e pour l'IA et l'indexation rapide des embeddings.

âœ… **Avantages** :
   -  FacilitÃ© d'utilisation avec une API simple.
   -  OptimisÃ© pour le stockage et la recherche rapide.
   -  IntÃ©gration native avec LangChain et d'autres frameworks IA.
     
ğŸ”— **Cas d'utilisation** : Projets nÃ©cessitant une solution lÃ©gÃ¨re et rapide pour la rÃ©cupÃ©ration d'informations.

ğŸ“Œ [ğŸ“ AccÃ©der a RAG avec ChromaDB fichier excel](https://github.com/GOHOUEDE/IA/blob/main/NLP-RAG/Fichier-excel-Chroma.py)

### 2ï¸âƒ£ **FAISS (Facebook AI Similarity Search)** ğŸš€
ğŸ”¹ **Description** : DÃ©veloppÃ© par Facebook AI, FAISS est une bibliothÃ¨que open-source dÃ©diÃ©e Ã  la recherche efficace sur de grands ensembles de donnÃ©es.

âœ… **Avantages** :
   -  TrÃ¨s rapide pour la recherche approximative.
   -  OptimisÃ© pour le traitement de grands volumes de donnÃ©es.
   -  Supporte des algorithmes avancÃ©s de quantification.
   - 
ğŸ”— **Cas d'utilisation** : SystÃ¨mes nÃ©cessitant une indexation ultra-rapide pour des bases volumineuses.

ğŸ“Œ [ğŸ“ AccÃ©der a RAG avec  FAISS fichier excel](https://github.com/GOHOUEDE/IA/blob/main/NLP-RAG/Fichier-excel-FAISS.py)

### 3ï¸âƒ£ **Qdrant** ğŸ†
ğŸ”¹ **Description** : Qdrant est un moteur de recherche vectorielle optimisÃ© pour la scalabilitÃ© et la recherche en temps rÃ©el.
âœ… **Avantages** :
   -  Performances Ã©levÃ©es sur les requÃªtes en temps rÃ©el.
   -  Support de la recherche hybride (mots-clÃ©s + vecteurs).
   -  Facile Ã  dÃ©ployer avec Docker et Kubernetes.
ğŸ”— **Cas d'utilisation** : Applications nÃ©cessitant une combinaison de recherche vectorielle et sÃ©mantique.

ğŸ“Œ [ğŸ“ AccÃ©der au fichier Qdrant]((https://github.com/GOHOUEDE/IA/blob/main/NLP-RAG/Fichier-excel-Qdrant.py)

## ğŸ§  Choix des Embeddings
Les embeddings transforment les textes en vecteurs numÃ©riques pour faciliter la recherche sÃ©mantique.

### ğŸ”¹ **OpenAI Embeddings** âš¡
-  Fournit des reprÃ©sentations vectorielles de haute qualitÃ©.
-  Compatible avec divers modÃ¨les d'OpenAI (GPT, CLIP, etc.).

### ğŸ”¹ **Sentence Transformers (SBERT)** ğŸ§©
-  OptimisÃ© pour des tÃ¢ches de similaritÃ© sÃ©mantique.
-  PrÃ©-entraÃ®nÃ© sur divers corpus pour une meilleure gÃ©nÃ©ralisation.

### ğŸ”¹ **Hugging Face Embeddings** ğŸ¤—
-  Large choix de modÃ¨les disponibles via la bibliothÃ¨que `transformers`.
-  FacilitÃ© d'intÃ©gration et d'adaptation selon les besoins spÃ©cifiques.


## ğŸ¤– Choix du ModÃ¨le LLM
Le modÃ¨le de langage joue un rÃ´le clÃ© dans l'interprÃ©tation et la gÃ©nÃ©ration des rÃ©ponses.

### ğŸ”¹ **GPT-4 (OpenAI)** ğŸš€
-  ModÃ¨le puissant avec des rÃ©ponses dÃ©taillÃ©es et prÃ©cises.
-  IntÃ©gration facile avec les API OpenAI.

### ğŸ”¹ **LLaMA (Meta AI)** ğŸ¦™
-  ModÃ¨le open-source performant.
-  NÃ©cessite une infrastructure adaptÃ©e pour l'hÃ©bergement.

### ğŸ”¹ **Mistral 7B** ğŸŒªï¸
-  Un modÃ¨le lÃ©ger et efficace pour des tÃ¢ches spÃ©cifiques.
-  Bon Ã©quilibre entre performance et coÃ»t d'infÃ©rence.

### ğŸ”¹ **Claude (Anthropic)** ğŸ¤
-  ConÃ§u pour des rÃ©ponses alignÃ©es et sÃ»res.
-  SpÃ©cialisÃ© dans les interactions naturelles et conversationnelles.

ğŸ“Œ [ğŸ“ AccÃ©der au fichier LLM](#)

## ğŸ¯ Conclusion
Le choix des composants dans un systÃ¨me **RAG** dÃ©pend du cas d'utilisation et des exigences en termes de **rapiditÃ©**, **prÃ©cision** et **scalabilitÃ©**. En combinant la bonne **base vectorielle**, les bons **embeddings** et un **LLM** adaptÃ©, il est possible de crÃ©er un systÃ¨me robuste et efficace pour la **rÃ©cupÃ©ration et la gÃ©nÃ©ration dâ€™informations intelligentes**. ğŸš€

ğŸ“Œ [ğŸ“ Voir tous les fichiers du projet](#)

